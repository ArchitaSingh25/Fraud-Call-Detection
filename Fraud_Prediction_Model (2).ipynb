{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278fec3f-3528-4006-b995-3d40bf3aaccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re              #package for importing regular expression\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import speech_recognition as sr\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f145fcc-47ea-4927-ade8-69c671df34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the input audio\n",
    "def audio_to_text():\n",
    "    conv=[]\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Listening stopped\")\n",
    "    try:  \n",
    "        text = recognizer.recognize_google(audio)\n",
    "        conv.append(text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand.\")\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf7f366-6097-461e-9286-72896d231c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to preprocess the data\n",
    "def preprocessing(dataset,num_of_rows=1):\n",
    "    stemmer =WordNetLemmatizer()\n",
    "    corpus=[]\n",
    "    #nltk.download('wordnet')\n",
    "    for i in range(0,num_of_rows):\n",
    "        #Removing words which are special character\n",
    "        document=re.sub(r'\\W',' ',dataset[i])\n",
    "        \n",
    "    \n",
    "        #Removing single characters from the document\n",
    "        document=re.sub(r'\\s+[a-zA-Z]\\s+',' ',document)\n",
    "    \n",
    "        #Removing single character from start\n",
    "        document=re.sub(r'\\^[a-zA-Z]\\s+',' ',document)\n",
    "    \n",
    "        #Removing one or more spaces and replacing by one space\n",
    "        document=re.sub(r'\\s+',' ',document,flags=re.I)\n",
    "    \n",
    "        document=document.lower()\n",
    "    \n",
    "        document=document.split()\n",
    "        document=[stemmer.lemmatize(w) for w in document]\n",
    "        document=' '.join(document)\n",
    "    \n",
    "        #Now adding it to our corpus\n",
    "        corpus.append(document)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe52ad07-6bd8-4acc-9636-f0d1bee47608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to train model 1\n",
    "def predict_using_count_vectoriser(dataset,num_of_rows,callData):\n",
    "    #Now we will divide the data for train and test our dataset\n",
    "    x=preprocessing(dataset['call_content'],num_of_rows)\n",
    "    y=dataset.Label   \n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "    #Testing the model over given conversation\n",
    "    #callData=[]\n",
    "    #callData=audio_to_text()\n",
    "    #callData=[\"Please don't text me anymore.\"]\n",
    "    live_test=preprocessing(callData,len(callData))\n",
    "    \n",
    "    # tokenizing the text data and counting the occurrences of each token using CountVectorizer\n",
    "    count_vectorizer=CountVectorizer(max_features=1500,min_df=5,max_df=0.7,stop_words=stopwords.words('english'))\n",
    "    \n",
    "    #fits the vectorizer to the documents (learns the vocabulary) and transforms the documents into matrix\n",
    "    count_train=count_vectorizer.fit_transform(x_train)\n",
    "    #only transform the document into matrix\n",
    "    count_test=count_vectorizer.transform(x_test)\n",
    "    live_count_test=count_vectorizer.transform(live_test)\n",
    "    \n",
    "    # Now with the help of Naive Bayes theorm we will fit the label\n",
    "    nbclassifier=MultinomialNB()\n",
    "    nbclassifier.fit(count_train,y_train)\n",
    "    y_pred=nbclassifier.predict(count_test)\n",
    "    live_y_pred=nbclassifier.predict(live_count_test)\n",
    "    score=metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "    cm=metrics.confusion_matrix(y_pred,y_test,labels=['normal','fraud'])\n",
    "    print(\"Accuracy score when using count vectoriser class: \",score)\n",
    "    print(\"CONFUSION MATRIX\\n\",cm)\n",
    "    return callData,live_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722641cf-9d29-490a-b8a4-145b702d84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to train model 2\n",
    "def predict_using_tfidf_vectoriser(dataset,num_of_rows,callData):\n",
    "    #Now we will divide the data for train and test our dataset\n",
    "    x=preprocessing(dataset['call_content'],num_of_rows)\n",
    "    y=dataset.Label   \n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)\n",
    "    #Testing the model over given conversation\n",
    "    #callData=[]\n",
    "    #callData=audio_to_text()\n",
    "    #callData=[\"Please don't text me anymore.\"]\n",
    "    live_x_test=preprocessing(callData,len(callData))\n",
    "    \n",
    "    #getting importance of a term (token) in a document\n",
    "    tfidfvectorizer=TfidfVectorizer(max_features=1500,min_df=5,max_df=0.7,stop_words=stopwords.words('english'))\n",
    "    \n",
    "    tfidf_train=tfidfvectorizer.fit_transform(x_train)\n",
    "    tfidf_test=tfidfvectorizer.transform(x_test)\n",
    "    live_tfidf_test=tfidfvectorizer.transform(live_x_test)\n",
    "    \n",
    "    # Now with the help of Naive Bayes theorm we will fit the label\n",
    "    nbclassifier=MultinomialNB()\n",
    "    nbclassifier.fit(tfidf_train,y_train)\n",
    "    y_predict=nbclassifier.predict(tfidf_test)\n",
    "    live_y_predict=nbclassifier.predict(live_tfidf_test)\n",
    "    score=metrics.accuracy_score(y_test,y_predict)\n",
    "    cm=metrics.confusion_matrix(y_predict,y_test,labels=['normal','fraud'])\n",
    "    print(\"Accuracy score when using tfidf vectoriser class ie weightage of a word: \",score)\n",
    "    print(\"CONFUSION MATRIX\\n\",cm)\n",
    "    return callData,live_y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d651d951-748f-46b2-9450-b6e27ee746ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Listening stopped\n"
     ]
    }
   ],
   "source": [
    "# Main calling\n",
    "# To load training dataset\n",
    "dataset=pd.read_csv(r\"D:\\Fraud_Call_Detection\\Fraud_calls.txt\",sep='|')\n",
    "#dataset.head()\n",
    "num_of_rows,y=dataset.shape\n",
    "callData=audio_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d1d074-1f29-4041-8ef3-037f83c2ebc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** USING COUNT VECTORISER **********\n",
      "\n",
      "Accuracy score when using count vectoriser class:  0.9735658042744657\n",
      "CONFUSION MATRIX\n",
      " [[1554   23]\n",
      " [  24  177]]\n",
      "Audio Received:['please give me your SBI credit card details'] \n",
      "predicted as label:['fraud']\n",
      "\n",
      " ********** USING TGIDF VECTORISER **********\n",
      "\n",
      "Accuracy score when using tfidf vectoriser class ie weightage of a word:  0.9780652418447694\n",
      "CONFUSION MATRIX\n",
      " [[1575   36]\n",
      " [   3  164]]\n",
      "Audio Received:['please give me your SBI credit card details']  \n",
      "predicted as label:['fraud']\n"
     ]
    }
   ],
   "source": [
    "# Training the model over Fraud_calls.txt dataset\n",
    "print(\"\\n ********** USING COUNT VECTORISER **********\\n\")\n",
    "conversation,label=predict_using_count_vectoriser(dataset,num_of_rows,callData)\n",
    "print(\"Audio Received:{} \\npredicted as label:{}\".format(conversation,label))\n",
    "\n",
    "print(\"\\n ********** USING TGIDF VECTORISER **********\\n\")\n",
    "conversation,label=predict_using_tfidf_vectoriser(dataset,num_of_rows,callData)\n",
    "print(\"Audio Received:{}  \\npredicted as label:{}\".format(conversation,label))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
